
# AI-Generated Voice Detection API (Multi-Language)

This project provides a RESTful API that determines whether an input voice sample is generated by artificial intelligence or spoken by a real human. The system is designed for automated evaluation, audio forensics, and anti-spoofing use cases.

---

## Overview

The API accepts a Base64-encoded MP3 audio sample and analyzes its acoustic characteristics using standard speech-processing techniques and a trained deep learning model. It returns a structured JSON response indicating whether the voice is **AI-generated** or **human**, along with a confidence score and explanation.


This is an end-to-end AI voice detection pipeline that classifies speech as human or AI-generated. It accepts Base64-encoded MP3 audio, enforces size limits,
and converts the input to 16 kHz mono WAV using FFmpeg (JavaCV). The service extracts MFCC features along with delta and delta-delta coefficients, applies Cepstral Mean and Variance Normalization (CMVN), 
and pads or truncates inputs to a fixed shape. A trained ONNX deep learning model performs inference, and results are calibrated using softmax and confidence stabilization to return a structured classification, 
confidence score, and human-readable explanation.
---

## System Architecture

1. A client sends a Base64-encoded audio file (MP3 format only) to the API.
2. The server decodes the audio and normalizes it for analysis.
3. Acoustic features are extracted from the audio signal.
4. A trained machine learning model estimates the likelihood of synthetic speech.
5. The API returns a structured JSON response containing classification and confidence.

---

## Feature Extraction

The system extracts the following acoustic features:

- **Mel-Frequency Cepstral Coefficients (MFCCs)**
- **Delta (Δ) coefficients**
- **Delta-Delta (ΔΔ) coefficients**
- **Cepstral Mean and Variance Normalization (CMVN)**

Together, these features form a compact and information-rich representation of speech suitable for:

- Speaker verification  
- Deepfake / AI voice detection  
- Audio forensics  
- Anti-spoofing systems  

The feature extraction process converts audio into numerical representations that capture how speech sounds and how it changes over time, while reducing the impact of background noise, recording devices, and channel variations. These techniques are standard in professional speech processing and forensic systems.

---

## Machine Learning Approach

The system employs a **deep learning-based feed-forward neural network** trained on MFCC, delta, and delta-delta acoustic features. The model performs **binary classification** to distinguish between human speech and AI-generated or synthetic speech.

The model is trained using supervised learning and optimized to generalize across different speakers and supported languages.

---

## API Specification

### Headers
x-api-key: <API_KEY>
Content-Type: application/json

---

### Request Body

```json
{
  "language": "English",
  "audioFormat": "mp3",
  "audioBase64": "<FULL_BASE64_AUDIO_STRING>"
}
Request Parameters

language: Language of the audio sample (e.g., English, Hindi, Tamil, Telugu, Malayalam)

audioFormat: Audio format (must be mp3)

audioBase64: Base64-encoded MP3 audio string (one audio per request)

Response Body (Success)
{
  "status": "success",
  "language": "English",
  "classification": "AI_GENERATED",
  "confidenceScore": 0.82,
  "explanation": "Voice produced by using AI or synthetic systems. Unnatural pitch consistency and robotic speech patterns detected."
}
Output Description

classification
Indicates whether the voice sample is AI_GENERATED or HUMAN.

confidenceScore
A floating-point value between 0.0 and 1.0 representing the model’s confidence in the prediction.

explanation
A short, human-readable description explaining the detection rationale.

API Request Example
curl -X POST https://<YOUR_API_URL>/api/voice-detection \
  -H "Content-Type: application/json" \
  -H "x-api-key: YOUR_API_KEY" \
  -d '{
        "language": "English",
        "audioFormat": "mp3",
        "audioBase64": "'"$(cat audio_base64.txt)"'"
      }'
Deployment

The API is deployed on Render using a Docker-based setup with CPU-only inference. This deployment approach ensures a stable, reproducible, and publicly accessible endpoint suitable for automated evaluation and integration testing.

Model Limitations

The model provides probabilistic predictions, not absolute determinations.

Performance may degrade on:

Extremely noisy recordings

Very short audio samples

Heavy compression artifacts

Unseen synthesis techniques not represented in training data

The system analyzes acoustic patterns only and does not verify speaker identity.

Classification confidence reflects model certainty, not legal or forensic proof.


